{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913260e-431d-474c-abf0-9156baeedff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bf68bd-d45b-4e70-9317-c7ef8cb79d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 16:04:12.699418: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 16:04:12.751947: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 16:04:13.676315: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "try:\n",
    "    import spacy\n",
    "except Exception as e:\n",
    "    spacy = None\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    SentenceTransformer = None\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fb0491-9175-4f76-929c-afe488c0ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Normalization & Regex Utils\n",
    "# -------------------------------\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Lowercase, normalize whitespace, remove control chars.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", text.strip().lower())\n",
    "\n",
    "\n",
    "STATUTE_REGEX = re.compile(r\"\"\"\n",
    "    (\n",
    "        (?:s\\.|sec(?:tion)?s?\\.?)\\s*\\d+[A-Za-z]?(?:[\\s,]*(?:and)?\\s*\\d+[A-Za-z]?)* |\n",
    "        (S\\.?\\s*\\d+[A-Za-z]?) |\n",
    "        (Section\\s+\\d+[A-Za-z]?(?:\\(\\d+\\))*) |\n",
    "        (Sec\\.?\\s*\\d+[A-Za-z]?) |\n",
    "        (Article\\s+\\d+[A-Za-z]?(?:\\(\\d+\\))*) |\n",
    "        (Art\\.?\\s*\\d+[A-Za-z]?) |\n",
    "        (Order\\s+[IVXLC]+\\s+Rule\\s+\\d+[A-Za-z]?) |\n",
    "        (Rule\\s+\\d+[A-Za-z]?(?:\\(\\d+\\))*) |\n",
    "        (Clause\\s*\\(?\\d+[A-Za-z]?\\)?) |\n",
    "        (\\b\\d{1,4}\\s+of\\s+\\d{4}\\b) |\n",
    "        (\\b\\d+\\s+SCC\\s+\\d+\\b) |\n",
    "        (\\bAIR\\s*\\[?\\d{4}\\]?\\s+[A-Z]+\\s+\\d+\\b)\n",
    "    )\n",
    "\"\"\", flags=re.I | re.X)\n",
    "\n",
    "CASE_CITATION_REGEX = re.compile(\n",
    "    r\"\\b([A-Z][\\w\\.\\-& ]{1,120}?\\s+v(?:\\.|s|ersus)\\.?\\s+[A-Z][\\w\\.\\-& ]{1,120}?)\\b\",\n",
    "    flags=re.I | re.X\n",
    ")\n",
    "\n",
    "\n",
    "def extract_statute_references(sentence: str) -> List[str]:\n",
    "    \"\"\"Extract statute/section references from a sentence.\"\"\"\n",
    "    matches = STATUTE_REGEX.findall(sentence)\n",
    "    return [\" \".join([x for x in m if x]).strip() if isinstance(m, tuple) else m.strip()\n",
    "            for m in matches if m]\n",
    "\n",
    "\n",
    "def extract_case_citations(sentence: str) -> List[str]:\n",
    "    \"\"\"Extract case citations from a sentence.\"\"\"\n",
    "    return [m.strip() for m in CASE_CITATION_REGEX.findall(sentence) if m]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. NLP & Embeddings\n",
    "# -------------------------------\n",
    "def load_spacy(model: str = \"en_core_web_sm\"):\n",
    "    \"\"\"Lazy-load spaCy model with helpful error if missing.\"\"\"\n",
    "    try:\n",
    "        return spacy.load(model)\n",
    "    except OSError:\n",
    "        raise OSError(f\"spaCy model '{model}' not found. Install with: python -m spacy download {model}\")\n",
    "\n",
    "\n",
    "def extract_entities_and_keyphrases(nlp, sentence: str, min_len: int = 2) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Return (entities, keyphrases) for a sentence.\"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    entities = [ent.text.strip() for ent in doc.ents if len(ent.text.strip()) >= min_len]\n",
    "    phrases = [chunk.text.strip() for chunk in doc.noun_chunks if len(chunk.text.strip()) >= min_len]\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    def unique(seq):\n",
    "        seen, out = set(), []\n",
    "        for x in seq:\n",
    "            k = x.lower()\n",
    "            if k not in seen:\n",
    "                seen.add(k)\n",
    "                out.append(x)\n",
    "        return out\n",
    "\n",
    "    return unique(entities), unique(phrases)\n",
    "\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \"\"\"Wrapper for SentenceTransformer sentence embeddings.\"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def encode(self, sentences: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "        sentences = [normalize_text(s) for s in sentences]\n",
    "        return np.array(self.model.encode(sentences, batch_size=batch_size, show_progress_bar=False))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Graph Construction\n",
    "# -------------------------------\n",
    "def construct_document_graph(\n",
    "    sentences: List[str],\n",
    "    nlp=None,\n",
    "    sent_embeddings: Optional[np.ndarray] = None,\n",
    "    emb_model: Optional[EmbeddingModel] = None,\n",
    "    sim_threshold: float = 0.7,\n",
    "    max_sim_edges: int = 3,\n",
    "    include_sequential_edges: bool = True,\n",
    "    include_semantic_edges: bool = True,\n",
    "    include_discourse_edges: bool = True,\n",
    ") -> Tuple[nx.DiGraph, Dict[str, Any]]:\n",
    "\n",
    "    # --- Normalize ---\n",
    "    sentences_clean = [normalize_text(s) for s in sentences]\n",
    "    num_sents = len(sentences_clean)\n",
    "\n",
    "    # --- Load spaCy if needed ---\n",
    "    if nlp is None:\n",
    "        try:\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except Exception:\n",
    "            nlp = None\n",
    "\n",
    "    # --- Embeddings ---\n",
    "    if sent_embeddings is None and emb_model:\n",
    "        sent_embeddings = emb_model.encode(sentences_clean)\n",
    "\n",
    "    # --- Graph Init ---\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Sentence nodes\n",
    "    for i, s in enumerate(sentences_clean):\n",
    "        G.add_node(f\"S_{i}\", type=\"sentence\", idx=i, text=s,\n",
    "                   emb=sent_embeddings[i] if sent_embeddings is not None else None)\n",
    "\n",
    "    # Virtual summary node\n",
    "    G.add_node(\"V_SUMMARY\", type=\"virtual\", text=\"<virtual_summary_node>\")\n",
    "\n",
    "    # --- Extract entities/phrases/statutes/cases ---\n",
    "    phrase_counter, sentence_entities, sentence_phrases = Counter(), [], []\n",
    "    for s in sentences_clean:\n",
    "        ents, phrs = ([], [])\n",
    "        if nlp:\n",
    "            try:\n",
    "                ents, phrs = extract_entities_and_keyphrases(nlp, s)\n",
    "            except Exception:\n",
    "                pass\n",
    "        if not ents:  # Fallback heuristic\n",
    "            ents = re.findall(r\"\\b([A-Z][a-z]{2,}(?:\\s+[A-Z][a-z]{2,})*)\\b\", s)\n",
    "\n",
    "        sentence_entities.append(ents)\n",
    "        sentence_phrases.append(phrs)\n",
    "        for p in set(ents + phrs):\n",
    "            phrase_counter[p.lower()] += 1\n",
    "\n",
    "    phrase_to_idx = {p: i for i, p in enumerate(phrase_counter.keys())}\n",
    "    for p, idx in phrase_to_idx.items():\n",
    "        G.add_node(f\"P_{idx}\", type=\"phrase\", text=p)\n",
    "\n",
    "    # --- Add Edges ---\n",
    "    # Sequential edges\n",
    "    if include_sequential_edges:\n",
    "        for i in range(num_sents - 1):\n",
    "            G.add_edge(f\"S_{i}\", f\"S_{i+1}\", type=\"sequential\", weight=1.0)\n",
    "\n",
    "    # Discourse edges\n",
    "    if include_discourse_edges:\n",
    "        discourse_markers = {\n",
    "            \"however\": \"contrast\", \"therefore\": \"result\", \"thus\": \"result\",\n",
    "            \"consequently\": \"result\", \"furthermore\": \"addition\",\n",
    "            \"moreover\": \"addition\", \"but\": \"contrast\", \"although\": \"contrast\"\n",
    "        }\n",
    "        for i, s in enumerate(sentences_clean):\n",
    "            for marker, dtype in discourse_markers.items():\n",
    "                if re.search(rf\"\\b{re.escape(marker)}\\b\", s.lower()) and i > 0:\n",
    "                    G.add_edge(f\"S_{i-1}\", f\"S_{i}\", type=f\"discourse_{dtype}\", weight=1.0)\n",
    "\n",
    "    # Phrase / citation edges\n",
    "    for i, (ents, phrs) in enumerate(zip(sentence_entities, sentence_phrases)):\n",
    "        sname = f\"S_{i}\"\n",
    "        statutes = extract_statute_references(sentences_clean[i])\n",
    "        cases = extract_case_citations(sentences_clean[i])\n",
    "\n",
    "        all_items = ents + phrs + statutes + cases\n",
    "        for item in all_items:\n",
    "            key = item.lower()\n",
    "            if key not in phrase_to_idx:\n",
    "                idx = len(phrase_to_idx)\n",
    "                phrase_to_idx[key] = idx\n",
    "                G.add_node(f\"P_{idx}\", type=\"phrase\", text=key)\n",
    "            pname = f\"P_{phrase_to_idx[key]}\"\n",
    "            G.add_edge(pname, sname, type=\"phrase_appears_in\", weight=1.0)\n",
    "            G.add_edge(sname, pname, type=\"sentence_mentions_phrase\", weight=1.0)\n",
    "\n",
    "    # Semantic similarity edges\n",
    "    if include_semantic_edges and sent_embeddings is not None:\n",
    "        sims = cosine_similarity(sent_embeddings)\n",
    "        for i in range(num_sents):\n",
    "            idxs = np.argsort(-sims[i])\n",
    "            added = 0\n",
    "            for j in idxs:\n",
    "                if i == j or sims[i, j] < sim_threshold:\n",
    "                    continue\n",
    "                G.add_edge(f\"S_{i}\", f\"S_{j}\", type=\"semantic\", weight=float(sims[i, j]))\n",
    "                added += 1\n",
    "                if added >= max_sim_edges:\n",
    "                    break\n",
    "\n",
    "    # Virtual summary connections\n",
    "    for i in range(num_sents):\n",
    "        sname = f\"S_{i}\"\n",
    "        G.add_edge(\"V_SUMMARY\", sname, type=\"virtual_to_sentence\", weight=0.1)\n",
    "        G.add_edge(sname, \"V_SUMMARY\", type=\"sentence_to_virtual\", weight=0.1)\n",
    "\n",
    "    meta = {\n",
    "        \"num_sentences\": num_sents,\n",
    "        \"sentences\": sentences_clean,\n",
    "        \"phrase_to_idx\": phrase_to_idx,\n",
    "        \"phrases\": list(phrase_to_idx.keys()),\n",
    "    }\n",
    "    return G, meta\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Dataset Wrapper\n",
    "# -------------------------------\n",
    "def build_graphs_for_dataset(\n",
    "    dataset: List[Dict[str, Any]],\n",
    "    nlp=None,\n",
    "    emb_model: Optional[EmbeddingModel] = None,\n",
    "    sim_threshold: float = 0.7,\n",
    "    max_sim_edges: int = 3,\n",
    ") -> List[Tuple[nx.DiGraph, Dict[str, Any]]]:\n",
    "    \"\"\"Build graphs for all documents in dataset.\"\"\"\n",
    "    graphs = []\n",
    "    for example in dataset:\n",
    "        sents = example.get(\"judgement_sent\") or example.get(\"judgement_sentences\") or example.get(\"sentences\", [])\n",
    "        G, meta = construct_document_graph(\n",
    "            sents, nlp=nlp, emb_model=emb_model,\n",
    "            sim_threshold=sim_threshold, max_sim_edges=max_sim_edges\n",
    "        )\n",
    "        graphs.append((G, meta))\n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad16f663-8d27-4e61-8e37-a170d0e541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Graph Transformer Layer ------------------\n",
    "class GraphTransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc_q = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        print(f\"[GraphTransformerLayer] Initialized with input_dim={input_dim}, hidden_dim={hidden_dim}\")\n",
    "\n",
    "    def forward(self, node_features, adj_matrix):\n",
    "        Q = self.fc_q(node_features)\n",
    "        K = self.fc_k(node_features)\n",
    "        V = self.fc_v(node_features)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(0, 1)) / (K.size(-1) ** 0.5)\n",
    "        scores = scores.masked_fill(adj_matrix == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "        # return F.layer_norm(out + node_features, out.shape[-1:])\n",
    "\n",
    "# ------------------ Graph Encoder ------------------\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphTransformerLayer(input_dim if i == 0 else hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, node_features, adj_matrix):\n",
    "        x = node_features\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, adj_matrix)\n",
    "        return x\n",
    "\n",
    "# ------------------ Graph to Sequence Attention ------------------\n",
    "class GraphToSeqAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, decoder_dim):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(hidden_dim, decoder_dim)\n",
    "        self.value = nn.Linear(hidden_dim, decoder_dim)\n",
    "        self.query = nn.Linear(decoder_dim, decoder_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, graph_node_features, decoder_hidden):\n",
    "        K = self.key(graph_node_features)       # [num_nodes, decoder_dim]\n",
    "        V = self.value(graph_node_features)     # [num_nodes, decoder_dim]\n",
    "        Q = self.query(decoder_hidden)          # [batch, seq_len, decoder_dim]\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(0, 1)) / (K.size(-1) ** 0.5)\n",
    "        attn_weights = self.softmax(attn_scores)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        return context, attn_weights\n",
    "\n",
    "# ------------------ Graph Enhanced Pegasus ------------------\n",
    "#NEW CODE\n",
    "import transformers\n",
    "class GraphEnhancedPegasus(nn.Module):\n",
    "    def __init__(self, pegasus_model_name=\"google/pegasus-large\", graph_hidden_dim=256, num_graph_layers=2, seed=42):\n",
    "        super().__init__()\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_name)\n",
    "        self.pegasus = PegasusForConditionalGeneration.from_pretrained(pegasus_model_name)\n",
    "        self.graph_encoder = GraphEncoder(input_dim=768, hidden_dim=graph_hidden_dim, num_layers=num_graph_layers)\n",
    "        self.graph_to_seq = GraphToSeqAttention(hidden_dim=graph_hidden_dim, decoder_dim=self.pegasus.config.d_model)\n",
    "        \n",
    "        # Project graph context to Pegasus hidden dimension\n",
    "        self.proj = nn.Linear(graph_hidden_dim, self.pegasus.config.d_model)\n",
    "\n",
    "        # deterministic generation\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        self.pegasus.config.do_sample = False\n",
    "\n",
    "    def forward(self,input_texts=None,graph_node_features=None,adj_matrix=None,labels=None,input_ids=None,attention_mask=None,max_length=60):\n",
    "        # Tokenize if raw text provided\n",
    "        if input_ids is None:\n",
    "            if input_texts is None:\n",
    "                raise ValueError(\"Provide either input_texts or input_ids\")\n",
    "            inputs = self.tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            input_ids = inputs.input_ids.to(graph_node_features.device)\n",
    "            attention_mask = inputs.attention_mask.to(graph_node_features.device)\n",
    "        else:\n",
    "            input_ids = input_ids.to(graph_node_features.device)\n",
    "            attention_mask = attention_mask.to(graph_node_features.device)\n",
    "        \n",
    "        # Pegasus encoder\n",
    "        encoder_outputs = self.pegasus.model.encoder(input_ids, attention_mask=attention_mask)\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Graph encoding\n",
    "        graph_hidden = self.graph_encoder(graph_node_features, adj_matrix)\n",
    "        \n",
    "        # Attend from decoder to graph\n",
    "        context, attn_weights = self.graph_to_seq(graph_hidden, encoder_hidden_states)\n",
    "        \n",
    "        # Add graph context\n",
    "        enhanced_encoder_states = encoder_hidden_states + context\n",
    "        \n",
    "        encoder_outputs = transformers.modeling_outputs.BaseModelOutput(\n",
    "            last_hidden_state=enhanced_encoder_states\n",
    "        )\n",
    "        seq_len = input_ids.size(1)\n",
    "        summary_ratio = 0.5  # generate summary ~30% of input tokens\n",
    "        max_length = max(10, int(seq_len * summary_ratio))\n",
    "        min_length = max(5, int(max_length * 0.5))\n",
    "        \n",
    "        if labels is None:\n",
    "            generated_ids = self.pegasus.generate(\n",
    "                input_ids=input_ids,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                attention_mask=attention_mask,\n",
    "                min_length=min_length,\n",
    "                max_length=max_length,\n",
    "                do_sample=False,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=1.2,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        else:\n",
    "            decoder_input_ids = self.pegasus.prepare_decoder_input_ids_from_labels(labels)\n",
    "            outputs = self.pegasus(\n",
    "                input_ids=None,\n",
    "                attention_mask=attention_mask,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            return outputs\n",
    "\n",
    "    def generate(self, input_ids=None, attention_mask=None, graph_node_features=None, adj_matrix=None, **kwargs):\n",
    "        encoder_outputs = self.pegasus.model.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "    \n",
    "        # Use same graph-to-seq attention as in forward()\n",
    "        graph_hidden = self.graph_encoder(graph_node_features, adj_matrix)\n",
    "        context, _ = self.graph_to_seq(graph_hidden, encoder_hidden_states)\n",
    "        enhanced_encoder_states = encoder_hidden_states + context\n",
    "    \n",
    "        encoder_outputs = transformers.modeling_outputs.BaseModelOutput(\n",
    "            last_hidden_state=enhanced_encoder_states\n",
    "        )\n",
    "    \n",
    "        safe_kwargs = {\n",
    "            k: v for k, v in kwargs.items()\n",
    "            if k not in [\"encoder_outputs\", \"inputs_embeds\"]\n",
    "        }\n",
    "    \n",
    "        # Step 5: Call Pegasus generate safely\n",
    "        return self.pegasus.generate(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            attention_mask=attention_mask,\n",
    "            **safe_kwargs  # user-specified params like max_length, num_beams, etc.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb436c5-f572-4a3f-a95f-94b81a12cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score evaluate bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e300aa-9f96-4ce3-925e-c58edb35c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate \n",
    "with open(\"data12.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193b611b-b590-440d-93e2-722a3b983d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLegalDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_len=1024, max_target_len=256, emb_dim=768):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_target_len = max_target_len\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        judgement = item[\"judgement_sent\"]\n",
    "        summary = \" \".join(item[\"headnote_sent\"])\n",
    "\n",
    "        # -------------------------------\n",
    "        # 1. Tokenize input\n",
    "        # -------------------------------\n",
    "        enc = self.tokenizer(\n",
    "            \" \".join(judgement),\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_input_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        dec = self.tokenizer(\n",
    "            summary,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_target_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------\n",
    "        # 2. Build document graph\n",
    "        # -------------------------------\n",
    "        # Use your build_graphs_for_dataset\n",
    "        graphs = build_graphs_for_dataset([item])\n",
    "        G, meta = graphs[0]\n",
    "\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        node_to_idx = {n: i for i, n in enumerate(G.nodes())}\n",
    "\n",
    "        # adjacency matrix\n",
    "        adj = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n",
    "        for u, v in G.edges():\n",
    "            adj[node_to_idx[u], node_to_idx[v]] = 1.0\n",
    "\n",
    "        adj_matrix = torch.tensor(adj)\n",
    "\n",
    "        # node features (placeholder: random embeddings, replace with actual if available)\n",
    "        node_features = torch.randn(num_nodes, self.emb_dim)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": dec[\"input_ids\"].squeeze(),\n",
    "            \"node_features\": node_features,\n",
    "            \"adj_matrix\": adj_matrix,\n",
    "            \"original_summary_text\": summary\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9daf4b1-2ea2-4010-adf8-068c56cce041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error during conversion: ChunkedEncodingError(ProtocolError('Response ended prematurely'))\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphTransformerLayer] Initialized with input_dim=768, hidden_dim=256\n",
      "[GraphTransformerLayer] Initialized with input_dim=256, hidden_dim=256\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = GraphLegalDataset(dataset[:8000], tokenizer)\n",
    "test_dataset  = GraphLegalDataset(dataset[8000:8500], tokenizer)\n",
    "\n",
    "model = GraphEnhancedPegasus(\n",
    "    pegasus_model_name=\"google/pegasus-large\",\n",
    "    graph_hidden_dim=256,\n",
    "    num_graph_layers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771ea6e-74b1-47b6-8973-1d85e2eea06c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DONE ON FIRST 8000 DATA FOR TRAINING\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(4):\n",
    "    print(f\"\\n=== Epoch {epoch+1} ===\")\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        node_features = batch[\"node_features\"][0].to(device)\n",
    "        adj_matrix = batch[\"adj_matrix\"][0].to(device)\n",
    "\n",
    "        with autocast():  # Mixed precision\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                graph_node_features=node_features,\n",
    "                adj_matrix=adj_matrix,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f2692-cb24-4f87-bfdc-711c39b905af",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./result1_graph_pegasus_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"{save_dir}/graph_pegasus_state_dict.pt\")\n",
    "\n",
    "model.pegasus.save_pretrained(save_dir)\n",
    "model.tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\" Model and tokenizer saved at: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f4a04-c24b-45fc-9c06-7b17f346570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./result1_graph_pegasus_model\"  # folder where you saved model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Initialize model with saved Pegasus directory\n",
    "model = GraphEnhancedPegasus(pegasus_model_name=save_dir)\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"graph_pegasus_state_dict.pt\"), map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\" Trained Graph-Enhanced Pegasus loaded.\")\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"result1_graph_pegasus_model\")\n",
    "test_dataset  = GraphLegalDataset(dataset[8000:8500], tokenizer)\n",
    "# Create test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions, references = [], []\n",
    "total_loss, total_tokens = 0, 0  # for perplexity\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=model.tokenizer.pad_token_id)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        node_features = batch[\"node_features\"][0].to(device)\n",
    "        adj_matrix = batch[\"adj_matrix\"][0].to(device)\n",
    "\n",
    "        seq_len = input_ids.size(1)\n",
    "        summary_ratio = 0.5  # generate summary of ~50% of input tokens\n",
    "        max_length = max(10, int(seq_len * summary_ratio))\n",
    "        min_length = max(5, int(max_length * 0.5))\n",
    "\n",
    "        # ---- Generation ----\n",
    "        generated_tokens = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            graph_node_features=node_features,\n",
    "            adj_matrix=adj_matrix,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=3,\n",
    "            repetition_penalty=2.0,\n",
    "            length_penalty=1.2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # ---- Decoding ----\n",
    "        preds = model.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        refs = model.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        references.extend(refs)\n",
    "\n",
    "        # Optional progress display\n",
    "        if step % 500 == 0:\n",
    "            print(f\"Test step {step}\")\n",
    "            print(\"Pred:\", preds[0][:200])\n",
    "            print(\"Ref :\", refs[0][:200])\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            graph_node_features=node_features,\n",
    "            adj_matrix=adj_matrix\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item() * labels.numel()\n",
    "        total_tokens += (labels != model.tokenizer.pad_token_id).sum().item()\n",
    "\n",
    "# -------------------------------\n",
    "#  Evaluation Metrics\n",
    "# -------------------------------\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# ROUGE\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# BLEU\n",
    "bleu_result = bleu.compute(\n",
    "    predictions=[p for p in predictions],\n",
    "    references=[r.split() for r in references]\n",
    ")\n",
    "\n",
    "# METEOR\n",
    "meteor_result = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "# BERTScore\n",
    "bertscore_result = bertscore.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    lang=\"en\"\n",
    ")\n",
    "# -------------------------------\n",
    "#  Perplexity Calculation\n",
    "# -------------------------------\n",
    "avg_loss = total_loss / total_tokens\n",
    "perplexity = math.exp(avg_loss) if avg_loss < 50 else float(\"inf\")\n",
    "\n",
    "# -------------------------------\n",
    "#  Display Results\n",
    "# -------------------------------\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(\"ROUGE:\", rouge_result)\n",
    "print(\"BLEU :\", bleu_result[\"bleu\"])\n",
    "print(\"METEOR:\", meteor_result[\"meteor\"])\n",
    "print(f\"BERTScore: P={sum(bertscore_result['precision'])/len(bertscore_result['precision']):.4f}, \"\n",
    "      f\"R={sum(bertscore_result['recall'])/len(bertscore_result['recall']):.4f}, \"\n",
    "      f\"F1={sum(bertscore_result['f1'])/len(bertscore_result['f1']):.4f}\")\n",
    "print(f\" Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47130585-9898-45ca-b745-04d414c80f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation Results ===\n",
    "# ROUGE: {'rouge1': np.float64(0.5832954886912554), 'rouge2': np.float64(0.3478273416488371), 'rougeL': np.float64(0.3866087180337574), 'rougeLsum': np.float64(0.38616374920897983)}\n",
    "# BLEU : 0.01892779342079829\n",
    "# METEOR: 0.4163490888882138\n",
    "# BERTScore: P=0.8725, R=0.8773, F1=0.8748\n",
    "# 🧠 Perplexity: 3.6866"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
